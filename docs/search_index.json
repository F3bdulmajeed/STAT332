[["index.html", "Regression Analysis Introduction", " Regression Analysis Lecturer: Abdulmajeed Alharbi Last update: 2022-02-21 Introduction This course is an introduction to applied data analysis. We will explore data sets, examine various regression models for the data, assess the validity of their assumptions, and determine which conclusions we can make (if any). Data analysis is a bit of an art; there may be several valid approaches. We will strongly emphasize the importance of critical thinking about the data and the question of interest. Our overall goal is to use a basic set of modeling tools to explore and analyze data and to present the results in a scientific report. We then consider simple linear regression, a model that uses only one predictor. After briefly reviewing some linear algebra, we turn to multiple linear regression, a model that uses multiple variables to predict the response of interest. For all models, we will examine the underlying assumptions. More specifically, do the data support the assumptions? Do they contradict them? What are the consequences for inference? Also, we will explore some nonlinear models and data transformations. Finally, we discuss Linear regression based on the categorical with some applications. The whole lecture note for this module can be found by clicking here. However, this textbook will be used for the exercise classes for the module (STAT332). The notes have not completed yet, I will still add new exercises and fix typos. If you find any typos or mistakes or have any question, I will appreciate if you email me at Aalharbi10@ksu.edu.sa Note: My colleague Abdulrahman Alfaifi is posting very helpful youtube videos discussing some exercises and past-exam questions you can reach that easily by clicking here. "],["st-tutorial.html", "1 1st Tutorial 1.1 Recap 1.2 Exercises 1.3 Coursework", " 1 1st Tutorial 1.1 Recap Simple linear regression in an important and fundamental technique in applied statistical modeling as it is used to describe the linearity in the relationship between two variables. Our model is: \\[y_i=\\beta_0+\\beta_1~x_i+e_i ~~~~~~~~\\text{where}~~~ i=1,2,...,n\\] Where \\(\\beta_0\\) bias parameter in the model and \\(\\beta_1\\) is the slope parameter, and the expected value of \\(y\\) is \\(E(y_i)=\\hat{y}= \\beta_0+\\beta_1~x_i\\). and the term \\(e\\) is referring to the error in our model. the bias and the slope parameter are still need to be estimated one important technique to do this estimation is by minimizing the sum of squared error \\(\\sum_{i=1}^{n} e_i^2\\). 1.2 Exercises Exercise 1.1 Find the least squared estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the regression parameters below: \\[y_i = \\beta_0 +\\beta_1~x_i+e_i\\] where \\(i =1,2, ... , n\\). Solution: To find the least squared estimators for \\(\\beta_0\\) and \\(\\beta_1\\) so we need to minimize the sum of squared error, i.e., \\[ Q = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 +\\beta_1~x_i))^2\\] over \\(\\beta_0\\) and \\(\\beta_1\\). In order to calculate this optimization we need to find the first derivative for \\(Q\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). To find the least squared estimator for \\(\\beta_1\\): \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\beta_1}~\\sum_{i=1}^{n} (y_i - (\\beta_0+\\beta_1~x_i))^2\\\\ &amp;= 2 \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 +\\beta_1~x_i))\\\\ &amp;= 2(\\sum_{i=1}^{n} x_i~y_i -~\\beta_0~ \\sum_{i=1}^{n}~x_i + ~\\beta_1~\\sum_{i=1}^{n}~x_i^2) \\end{align}\\] Now by setting that to zero and solve we will end up with: \\[\\begin{align} \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\end{align}\\] To find the least squared estimator for \\(\\beta_0\\): \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\beta_0}~\\sum_{i=1}^{n} (y_i - (\\beta_0+\\beta_1~x_i))^2\\\\ &amp;= \\sum_{i=1}^{n} (y_i - (\\beta_0 +\\beta_1~x_i))\\\\ &amp;= n\\bar{y} -n\\beta_0 - n~\\beta_1~\\bar{x} \\end{align}\\] again by setting that to zero and solve we will end up with: \\[\\begin{align} \\hat{\\beta_0} = \\bar{y} -\\beta_1~\\bar{x} \\end{align}\\] Exercise 1.2 Suppose a model states that: \\[\\begin{align} &amp;E(y_1) = \\theta,\\\\ &amp;E(y_2) = 2~\\theta  \\phi,\\\\ &amp;E(y_3) = \\theta + 2~\\phi. \\end{align}\\] Find the least squares estimates of \\(\\theta\\) and \\(\\phi\\). Solution: Similarly to what we have done in the previous exercise we need to minimize the sum of squared error over \\(\\theta\\) and \\(\\phi\\). Lets begin by estimating \\(\\theta\\). \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\theta}~\\sum_{i=1}^{n}(y_i - E(y_i))^2\\\\ =&amp; \\frac{\\partial}{\\partial~\\theta} ((y_1 - \\theta)^2 +(y_2 -2\\theta + \\phi)^2 +(y_3-\\theta-2\\phi)^2)\\\\ =&amp; -2(y_1 - \\theta) - 4(y_2 -2\\theta + \\phi) - 2(y_3-\\theta-2\\phi)\\\\ =&amp; -2y_1 + 2\\theta -4y_2+8\\theta-4\\phi -2y_3+2\\theta+4\\phi\\\\ =&amp; 12\\theta - 2y_1-4y_2-2y_3 \\end{align}\\] Therefore, \\[\\hat{\\theta}= \\frac{y_3+2y_2+y_1}{6}\\] Now, the least square estimator for \\(\\phi\\) can be founded as follow: \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\phi}~\\sum_{i=1}^{n}(y_i - E(y_i))^2\\\\ =&amp; \\frac{\\partial}{\\partial~\\phi} ((y_1 - \\theta)^2 +(y_2 -2\\theta + \\phi)^2 +(y_3-\\theta-2\\phi)^2)\\\\ =&amp; 2(y_2 -2\\theta + \\phi) - 4(y_3-\\theta-2\\phi)\\\\ =&amp; 2y_2 -4\\theta +2\\phi -4y_3+4\\theta+8\\phi\\\\ =&amp; 10\\phi + 2y_2-4y_3 \\end{align}\\] Therefore, \\[\\hat{\\phi}= \\frac{2y_3-y_2}{5}\\] Exercise 1.3 Under certain conditions, the yield (Y) of a chemical process can be described via the relationship \\[Y = \\alpha~X^\\beta\\] where \\(X\\)is the amount of catalyst provided. Assume that the log of the yield is normally distributed and estimate the parameters \\(\\alpha\\) and \\(\\beta\\) for the data given below. X 0.500 0.600 0.700 0.800 0.900 1.00 Y 0.241 0.445 0.667 1.044 1.358 2.01 Solution: As \\[Y = \\alpha~X^\\beta\\] \\(\\Rightarrow\\) \\(\\log(Y)=log(\\alpha)+\\beta\\log(X)\\) We assume \\[Y&#39;_i= \\alpha&#39;+\\beta~X&#39;_i+e_i ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ i=1,2,..,n \\] Let \\(\\hat{\\alpha&#39;}\\) and \\(\\hat{\\beta}\\) is the least squared estimators for \\(\\alpha&#39;\\) and \\(\\beta\\) respectively, so \\[\\hat{\\beta}=\\frac{S_{X&#39;~Y&#39;}}{S_{X&#39;X&#39;}} ~~~~~~~ \\text{and} ~~~~~ \\hat{\\alpha&#39;}= \\bar{Y&#39;}-\\hat{\\beta}\\bar{X&#39;}\\] We can calculate that by R as follow: X=c(0.5,0.6,0.7,0.8,0.9,1) Y=c(0.241,0.445,0.667,1.044,1.358,2.010) Xdash= log(X) Ydash= log(Y) betahat = cov(Xdash,Ydash)/var(Xdash) alpha_dash_hat = mean(Ydash)- betahat * mean(Ydash) print(paste(&quot;The least squared estimator for beta is&quot;, betahat, &quot;and the least squared estimator for log alpha is&quot;, alpha_dash_hat)) ## [1] &quot;The least squared estimator for beta is 2.99310690670613 and the least squared estimator for log alpha is 0.528305318701578&quot; Now we can calculate, so \\(\\hat{\\alpha}= e^{0.53} = 1.7\\) 1.3 Coursework Watch the Youtube video by clicking here Look at iris data in R using the command help(iris) and fit a line to predict the petel length using the sepal length and interpret the result (you can use the function lm()) Show that \\(\\hat{\\beta_1}\\) is an unbiased estimate of for \\(\\beta_1\\). "],["nd-tutorial.html", "2 2nd Tutorial 2.1 Recap 2.2 Exercises 2.3 Coursework", " 2 2nd Tutorial 2.1 Recap Previously we discussed the least squared approach to estimate the slope and the bias parameters in the simple linear model. An important question arises here is to test how significant is our estimation and in order to assess that let us assume the error term is normally distributed with zero mean and constant variance \\(\\sigma^2\\). We want to test \\(H_0 : \\beta_j = \\hat{\\beta}~~~ \\text{vs.}~ ~~H_1 : \\beta_{j}= \\hat{\\beta}\\) at level \\(\\alpha\\) where \\(\\hat{\\beta}\\) is some constant and \\(j=0,1\\). The decision rule is to reject \\(H_0\\) if: \\[|T|=\\lvert~\\frac{\\beta_j-{\\hat\\beta}}{SE(\\hat\\beta_j)}\\lvert~&gt; t_{n-2,\\alpha/2}\\] Where \\(SE()\\) is the standard error of the parameter. Recall that from the lecture note: \\[\\begin{align} &amp;Var(\\beta_1)=\\frac{\\sigma^2}{(n-1)S_{X&#39;X&#39;}} = \\frac{\\sigma^2}{\\sum (x_i-\\bar{x})2}\\\\ &amp;Var(\\beta_0)= \\sigma^2 (\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum (x_i-\\bar{x})2}) \\end{align}\\] 2.2 Exercises Exercise 2.1 Go back to Exercise 1.3 and test the hypothesis that the yield is proportional to the amount of catalyst cubed. Solution 2.1: Now \\(Var(\\beta)=\\frac{\\sigma^2}{(n-1)S_{X&#39;X&#39;}}\\) \\[(n-1)S_{X&#39;X&#39;}=\\sum_{i=1}^{n}x&#39;^2_i-\\frac{(\\sum_{i=1}^{n}x&#39;_i)^2}{n}=0.33\\] While \\(\\sigma^2\\) can be estimated by using the unbiased estimator \\(s^2\\), \\[s^2=\\frac{1}{2}\\sum_{i=1}^{n}(y&#39;_i-\\hat{y&#39;_i})^2=0.0018\\] Hence \\(SE(\\hat{\\beta})=\\sqrt{Var(\\hat\\beta)}= 0.0739\\). Now to test \\(H_0: \\beta=3\\) against \\(H_q: \\beta\\neq3\\) Under \\(H_0\\), the T-test is \\(-0.0932\\) So the p-value is \\(P(|t_4| &gt; 0.0932) = 2P(t_4 &lt; 0.0932) = 0.9302\\). There is therefore no evidence to reject \\(H_0\\) i.e. no evidence against a cubic relationship. Exercise 2.2: The results of a class of 10 students on midterm marks \\(X\\) and on the final marks \\(Y\\) are as follows: X 77 54 71 72 81 94 96 99 83 67 Y 82 38 78 34 37 85 99 99 79 67 Calculate the regression line by hand. interpret the result. Construct \\(95\\%\\) confidence intervals for the bias and the weight coefficients. Use the lm() function in R to calculate regression line. Use the confint() in R to construct \\(95\\%\\) confidence intervals for the model coefficients. plot the regression line using the function plot(). If a student got in his midterm \\(50\\) what the expected mark in his final exam? Solution 2.2: Now we can estimate the regression parameters using the least square approach so, \\[\\begin{align} &amp;\\hat{\\beta_1} = \\frac{\\sum(y-\\bar{y})(x-\\bar{x})}{\\sum(x-\\bar{x})^2} = \\frac{2268.8}{ 1818.4}= 1.24769 \\\\ &amp;\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta}_1~\\bar{x} = -29.26659 \\end{align}\\] the interpretation of the coefficients is that the model expect that the final mark is increase by \\(1.25\\) when the midterm mark increase by ONE while \\(\\beta_0\\) is the y-axis intercept. To construct a \\(95\\%\\) confidence interval we know that \\[ |T|=\\lvert~\\frac{\\beta_j-{\\hat\\beta}}{SE(\\hat\\beta_j)}\\lvert \\sim t_{n-2}\\] It follows that a \\(100(1  )%\\) confidence interval for \\(\\beta_j\\) is \\[\\hat{\\beta_j} \\pm SE(\\hat\\beta_j)\\times~t_{n-2,\\alpha/2}\\] So as \\[\\begin{align} &amp;\\sigma^2 = \\frac{1}{n-2}~\\sum~(y_i-\\hat{y})^2 = \\frac{1}{8}~\\sum (y_i-(1.24769~x_i29.26659))^2 = 347.855\\\\ &amp;SE(\\hat\\beta_1) =\\sqrt{\\frac{\\sigma^2}{\\sum{(x_i-\\bar{x})^2}}} = \\sqrt{\\frac{347.855}{1818.4}}=\\sqrt{0.1912973}=0.4373755\\\\ &amp;SE(\\hat{\\beta_0})=\\sqrt{\\sigma^2~(\\frac{1}{n}+\\frac{\\bar{x}}{\\sum{(x_i-\\bar{x})^2}})}= \\sqrt{347.855(\\frac{1}{8}+\\frac{6304.36}{1818.4})} = 35.34811 \\end{align}\\] Therefore, the \\(95\\%\\) confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\begin{align} \\beta_0 &amp;\\in (-110.495,51.9621)\\\\ \\beta_1 &amp;\\in (0.239,2.256) \\end{align}\\] In R we can use the build-in function lm() as follow: X=c(77,54,71,72,81,94,96,99,83,67) Y=c(82,38,78,34,37,85,99,99,79,67) fit &lt;- lm(Y~X) summary(fit) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.796 -2.289 4.727 11.626 18.681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.2666 35.2249 -0.831 0.4302 ## X 1.2477 0.4374 2.853 0.0214 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.65 on 8 degrees of freedom ## Multiple R-squared: 0.5043, Adjusted R-squared: 0.4423 ## F-statistic: 8.138 on 1 and 8 DF, p-value: 0.02139 Then confint() can be used to find \\(95\\%\\) confidence interval as follow: confint(fit) ## 2.5 % 97.5 % ## (Intercept) -110.4953424 51.96213 ## X 0.2391006 2.25628 plot(X,Y) abline(fit) To predict the final mark for the student whom midterm mark \\(55\\) predict(fit,newdata = data.frame(X=c(55))) ## 1 ## 39.35636 2.3 Coursework 1 . The computer repair data gives the length of time of service calls in minutes \\(Y\\) and the number of components repaired in a computer \\(X\\). Some summary measures for this data are: \\[\\begin{align} &amp;n=14\\\\ &amp;\\sum x_i=84\\\\ &amp;\\sum y_i=1361\\\\ &amp;S_{XX} = 114\\\\ &amp;S_{YY} = 27768.26\\\\ &amp;S_{XY} = 1768 \\end{align}\\] Find the point estimate of the intercept and slop to model the length of service call as a linear function of the number of units serviced. Give 95% confidence interval for the slope and interpret. "],["rd-tutorial.html", "3 3rd Tutorial 3.1 Recap 3.2 Exercieses", " 3 3rd Tutorial 3.1 Recap We want to test \\(H_0:\\beta_0 = \\beta_1 = 0\\) vs. \\(H_1 :\\beta_1~~ \\text{or} ~~\\beta_0 \\neq 0\\) at level \\(\\alpha\\). Note that if we reject \\(H_0\\) we are saying that the model \\(\\hat{\\beta}_0 +\\hat{\\beta_1}X\\) has some ability to explain the variance that we are observing in Y . (i.e. There exists a linear relationship between the explanatory variables and the response variable.) Recall that ANOVA table can be used in the test for the existence of regression which is Source of Variation SS df MS F Regression SSR 1 MSR \\(F_0=\\frac{MSR}{MSE}\\) Error SSE n-2 MSE Total SSTO n-1 Where, \\[\\begin{align} SSR &amp;= \\sum~(\\hat{y}_{i}-\\bar{y})^2\\\\ SSE &amp;= \\sum~(\\hat{y_i}-y_i)^2\\\\ SSTO&amp;= \\sum~(y_i-\\bar{y})^2\\\\ MSR &amp;= \\frac{SSR}{1}\\\\ MSE &amp;= \\frac{SSE}{n-2} \\end{align}\\] To assess how well the regression line fit the data one can use the The coefficient of determination \\(R^2\\) which can be defined as follow: \\[R^2=\\frac{SSR}{SSTO}=1-~\\frac{SSE}{SSTO}\\] which measures the proportion of variability explained by the regression. 3.2 Exercieses In the following R output 9 values denoted {i?} for \\(i = 1,2\\) ,, \\(9\\) have been removed. What are the \\(9\\) missing values? Note: there are 50 observations are used to draw this model. interpret \\(R^2\\) in the previous Exercise. Draw a conclusion about the existence of the regression. 2. A second-hand cars dealer has 10 cars for sale. He decides to investigate the relation between the cars age \\(X\\) (in years) and the millage \\(Y\\) (in thousands miles) by using the simple linear regression model. The dealer reported the following: The mean and standard deviation of the cars millage are given, respectively, by \\(40.6\\) and \\(11.87153\\). The correlation coefficient between \\(X\\) and \\(Y\\) is \\(0.9687105\\). The estimated simple regression model is \\(\\hat{Y} = 8.892 + 7.733X\\). where \\[ = 10 ~~~~~~~~~~ \\bar{Y} = 40.6 ~~~~~~~~~~ S_Y = 11.87153 \\] Obtain \\(S_{YY} , S_{XX}\\) and \\(S_{XY}\\) Construct \\(90\\%\\) CI for the slope Compute the \\(95\\%\\) CI for car millage with age \\(7\\) years. Use ANOVA for testing the significance of the linearity. What proportion of the total variation in millage is explained by age? ## Coursework "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
