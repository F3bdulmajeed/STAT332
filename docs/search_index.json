[["index.html", "Regression Analysis Introduction", " Regression Analysis Lecturer: Abdulmajeed Alharbi Last update: 2022-02-16 Introduction This course is an introduction to applied data analysis. We will explore data sets, examine various regression models for the data, assess the validity of their assumptions, and determine which conclusions we can make (if any). Data analysis is a bit of an art; there may be several valid approaches. We will strongly emphasize the importance of critical thinking about the data and the question of interest. Our overall goal is to use a basic set of modeling tools to explore and analyze data and to present the results in a scientific report. We then consider simple linear regression, a model that uses only one predictor. After briefly reviewing some linear algebra, we turn to multiple linear regression, a model that uses multiple variables to predict the response of interest. For all models, we will examine the underlying assumptions. More specifically, do the data support the assumptions? Do they contradict them? What are the consequences for inference? Also, we will explore some nonlinear models and data transformations. Finally, we discuss Linear regression based on the categorical with some applications. The whole lecture note for this module can be found by clicking here. However, this textbook will be used for the exercise classes for the module (STAT332). The notes have not completed yet, I will still add new exercises and fix typos. If you find any typos or mistakes or have any question, I will appreciate if you email me at Aalharbi10@ksu.edu.sa Note: My fellow Abdulrahman Alfaifi is posting very helpful youtube videos discussing some exercises and past-exam questions you can reach that easily by clicking here. "],["least-squared-estimation.html", "1 Least squared estimation 1.1 Recap 1.2 Exercises 1.3 Coursework", " 1 Least squared estimation 1.1 Recap Simple linear regression in an important and fundamental technique in applied statistical modeling as it is used to describe the linearity in the relationship between two variables. Our model is: \\[y_i=\\beta_0+\\beta_1~x_i+e_i ~~~~~~~~\\text{where}~~~ i=1,2,...,n\\] Where \\(\\beta_0\\) bias parameter in the model and \\(\\beta_1\\) is the slope parameter, and the expected value of \\(y\\) is \\(E(y_i)=\\hat{y}= \\beta_0+\\beta_1~x_i\\). and the term \\(e\\) is referring to the error in our model. the bias and the slope parameter are still need to be estimated one important technique to do this estimation is by minimizing the sum of squared error \\(\\sum_{i=1}^{n} e_i^2\\). 1.2 Exercises Exercise 1.1 Find the least squared estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the regression parameters below: \\[y_i = \\beta_0 +\\beta_1~x_i+e_i\\] where \\(i =1,2, ... , n\\). Solution: To find the least squared estimators for \\(\\beta_0\\) and \\(\\beta_1\\) so we need to minimize the sum of squared error, i.e., \\[ Q = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 +\\beta_1~x_i))^2\\] over \\(\\beta_0\\) and \\(\\beta_1\\). In order to calculate this optimization we need to find the first derivative for \\(Q\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). To find the least squared estimator for \\(\\beta_1\\): \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\beta_1}~\\sum_{i=1}^{n} (y_i - (\\beta_0+\\beta_1~x_i))^2\\\\ &amp;= 2 \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 +\\beta_1~x_i))\\\\ &amp;= 2(\\sum_{i=1}^{n} x_i~y_i -~\\beta_0~ \\sum_{i=1}^{n}~x_i + ~\\beta_1~\\sum_{i=1}^{n}~x_i^2) \\end{align}\\] Now by setting that to zero and solve we will end up with: \\[\\begin{align} \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\end{align}\\] To find the least squared estimator for \\(\\beta_0\\): \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\beta_0}~\\sum_{i=1}^{n} (y_i - (\\beta_0+\\beta_1~x_i))^2\\\\ &amp;= \\sum_{i=1}^{n} (y_i - (\\beta_0 +\\beta_1~x_i))\\\\ &amp;= n\\bar{y} -n\\beta_0 - n~\\beta_1~\\bar{x} \\end{align}\\] again by setting that to zero and solve we will end up with: \\[\\begin{align} \\hat{\\beta_0} = \\bar{y} -\\beta_1~\\bar{x} \\end{align}\\] Exercise 1.2 Suppose a model states that: \\[\\begin{align} &amp;E(y_1) = \\theta,\\\\ &amp;E(y_2) = 2~\\theta  \\phi,\\\\ &amp;E(y_3) = \\theta + 2~\\phi. \\end{align}\\] Find the least squares estimates of \\(\\theta\\) and \\(\\phi\\). Solution: Similarly to what we have done in the previous exercise we need to minimize the sum of squared error over \\(\\theta\\) and \\(\\phi\\). Lets begin by estimating \\(\\theta\\). \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\theta}~\\sum_{i=1}^{n}(y_i - E(y_i))^2\\\\ =&amp; \\frac{\\partial}{\\partial~\\theta} ((y_1 - \\theta)^2 +(y_2 -2\\theta + \\phi)^2 +(y_3-\\theta-2\\phi)^2)\\\\ =&amp; -2(y_1 - \\theta) - 4(y_2 -2\\theta + \\phi) - 2(y_3-\\theta-2\\phi)\\\\ =&amp; -2y_1 + 2\\theta -4y_2+8\\theta-4\\phi -2y_3+2\\theta+4\\phi\\\\ =&amp; 12\\theta - 2y_1-4y_2-2y_3 \\end{align}\\] Therefore, \\[\\hat{\\theta}= \\frac{y_3+2y_2+y_1}{6}\\] Now, the least square estimator for \\(\\phi\\) can be founded as follow: \\[\\begin{align} &amp;\\frac{\\partial}{\\partial~\\phi}~\\sum_{i=1}^{n}(y_i - E(y_i))^2\\\\ =&amp; \\frac{\\partial}{\\partial~\\phi} ((y_1 - \\theta)^2 +(y_2 -2\\theta + \\phi)^2 +(y_3-\\theta-2\\phi)^2)\\\\ =&amp; 2(y_2 -2\\theta + \\phi) - 4(y_3-\\theta-2\\phi)\\\\ =&amp; 2y_2 -4\\theta +2\\phi -4y_3+4\\theta+8\\phi\\\\ =&amp; 10\\phi + 2y_2-4y_3 \\end{align}\\] Therefore, \\[\\hat{\\phi}= \\frac{2y_3-y_2}{5}\\] Exercise 1.3 Under certain conditions, the yield (Y) of a chemical process can be described via the relationship \\[Y = \\alpha~X^\\beta\\] where \\(X\\)is the amount of catalyst provided. Assume that the log of the yield is normally distributed and estimate the parameters \\(\\alpha\\) and \\(\\beta\\) for the data given below. X 0.500 0.600 0.700 0.800 0.900 1.00 Y 0.241 0.445 0.667 1.044 1.358 2.01 Solution: As \\[Y = \\alpha~X^\\beta\\] \\(\\Rightarrow\\) \\(\\log(Y)=log(\\alpha)+\\beta\\log(X)\\) We assume \\[Y&#39;_i= \\alpha&#39;+\\beta~X&#39;_i+e_i ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ i=1,2,..,n \\] Let \\(\\hat{\\alpha&#39;}\\) and \\(\\hat{\\beta}\\) is the least squared estimators for \\(\\alpha&#39;\\) and \\(\\beta\\) respectively, so \\[\\hat{\\beta}=\\frac{S_{X&#39;~Y&#39;}}{S_{X&#39;X&#39;}} ~~~~~~~ \\text{and} ~~~~~ \\hat{\\alpha&#39;}= \\bar{Y&#39;}-\\hat{\\beta}\\bar{X&#39;}\\] We can calculate that by R as follow: X=c(0.5,0.6,0.7,0.8,0.9,1) Y=c(0.241,0.445,0.667,1.044,1.358,2.010) Xdash= log(X) Ydash= log(Y) betahat = cov(Xdash,Ydash)/var(Xdash) alpha_dash_hat = mean(Ydash)- betahat * mean(Ydash) print(paste(&quot;The least squared estimator for beta is&quot;, betahat, &quot;and the least squared estimator for log alpha is&quot;, alpha_dash_hat)) ## [1] &quot;The least squared estimator for beta is 2.99310690670613 and the least squared estimator for log alpha is 0.528305318701578&quot; Now we can calculate, so \\(\\hat{\\alpha}= e^{0.53} = 1.7\\) 1.3 Coursework Watch the Youtube video by clicking here Look at iris data in R using the command help(iris) and fit a line to predict the petel length using the sepal length and interpret the result (you can use the function lm()) Show that \\(\\hat{\\beta_1}\\) is an unbiased estimate of for \\(\\beta_1\\). "],["properties-and-inferences-in-regression.html", "2 Properties and inferences in Regression 2.1 Recap 2.2 Exercises 2.3 Coursework", " 2 Properties and inferences in Regression 2.1 Recap Previously we discussed the least squared approach to estimate the slope and the bias parameters in the simple linear model. An important question arises here is to test how significant is our estimation and in order to assess that let us assume the error term is normally distributed with zero mean and constant variance \\(\\sigma^2\\). We want to test \\(H_0 : \\beta_j = \\hat{\\beta}~~~ \\text{vs.}~ ~~H_1 : \\beta_{j}= \\hat{\\beta}\\) at level \\(\\alpha\\) where \\(\\hat{\\beta}\\) is some constant and \\(j=0,1\\). The decision rule is to reject \\(H_0\\) if: \\[|T|=\\lvert~\\frac{\\beta_j-{\\hat\\beta}}{SE(\\hat\\beta_j)}\\lvert~&gt; t_{n-2,\\alpha/2}\\] Where \\(SE()\\) is the standard error of the parameter. Recall that from the lecture note: \\[\\begin{align} &amp;Var(\\beta_1)=\\frac{\\sigma^2}{(n-1)S_{X&#39;X&#39;}} = \\frac{\\sigma^2}{\\sum (x_i-\\bar{x})2}\\\\ &amp;Var(\\beta_0)= \\sigma^2 (\\frac{1}{n}+\\frac{\\bar{X}^2}{\\sum (x_i-\\bar{x})2}) \\end{align}\\] 2.2 Exercises Exercise 2.1 Go back to Exercise 1.3 and test the hypothesis that the yield is proportional to the amount of catalyst cubed. Solution 2.1: Now \\(Var(\\beta)=\\frac{\\sigma^2}{(n-1)S_{X&#39;X&#39;}}\\) \\[(n-1)S_{X&#39;X&#39;}=\\sum_{i=1}^{n}x&#39;^2_i-\\frac{(\\sum_{i=1}^{n}x&#39;_i)^2}{n}=0.33\\] While \\(\\sigma^2\\) can be estimated by using the unbiased estimator \\(s^2\\), \\[s^2=\\frac{1}{2}\\sum_{i=1}^{n}(y&#39;_i-\\hat{y&#39;_i})^2=0.0018\\] Hence \\(SE(\\beta)=\\sqrt{Var(\\hat\\beta)}= 0.0739\\). Now to test \\(H_0: \\beta=3\\) against \\(H_q: \\beta\\neq3\\) Under \\(H_0\\), the T-test is \\(-0.0932\\) So the p-value is \\(P(|t4| &gt; 0.0932) = 2P(t4 &lt; 0.0932) = 0.9302\\). There is therefore no evidence to reject \\(H_0\\) i.e. no evidence against a cubic relationship. Exercise 2.2: The results of a class of 10 students on midterm marks \\(X\\) and on the final marks \\(Y\\) are as follows: X 77 54 71 72 81 94 96 99 83 67 Y 82 38 78 34 37 85 99 99 79 67 Calculate the regression line by hand. interpret the result. Construct \\(95\\%\\) confidence intervals for the bias and the slope coefficients and explain the results.. Use the lm() function in R to calculate regression line. Use the confint() in R to construct \\(95\\%\\) confidence intervals for the model coefficients. plot the regression line using the function plot(). If a student got in his midterm \\(50\\) what the expected mark in his final exam? 2.3 Coursework "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
